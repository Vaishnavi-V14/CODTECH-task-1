# Importing required libraries
import numpy as np
import pandas as pd
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
import matplotlib.pyplot as plt

# Step 1: Load the dataset
data = load_iris()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = pd.Series(data.target)

# Step 2: Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Step 3: Define models to compare
models = {
    "Logistic Regression": LogisticRegression(max_iter=200),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42)
}

# Step 4: Train and evaluate models
results = {}
for model_name, model in models.items():
    # Train the model
    model.fit(X_train, y_train)
    
    # Make predictions
    y_pred = model.predict(X_test)
    
    # Evaluate the model
    results[model_name] = {
        "Accuracy": accuracy_score(y_test, y_pred),
        "Precision": precision_score(y_test, y_pred, average='weighted'),
        "Recall": recall_score(y_test, y_pred, average='weighted'),
        "F1 Score": f1_score(y_test, y_pred, average='weighted'),
        "Confusion Matrix": confusion_matrix(y_test, y_pred)
    }

# Step 5: Display the results
for model_name, metrics in results.items():
    print(f"Model: {model_name}")
    for metric, value in metrics.items():
        if metric == "Confusion Matrix":
            print(f"{metric}:\n{value}")
        else:
            print(f"{metric}: {value:.4f}")
    print("-" * 30)

# Step 6: Plot the comparison
metrics_to_plot = ["Accuracy", "Precision", "Recall", "F1 Score"]
bar_width = 0.4
x = np.arange(len(metrics_to_plot))

# Create a grouped bar chart
fig, ax = plt.subplots()
for i, (model_name, metrics) in enumerate(results.items()):
    ax.bar(x + i * bar_width, [metrics[metric] for metric in metrics_to_plot], bar_width, label=model_name)

# Add labels and title
ax.set_xlabel("Metrics")
ax.set_ylabel("Scores")
ax.set_title("Model Evaluation and Comparison")
ax.set_xticks(x + bar_width / 2)
ax.set_xticklabels(metrics_to_plot)
ax.legend()

# Show plot
plt.tight_layout()
plt.show()
